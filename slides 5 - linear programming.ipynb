{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up environment with correct dependencies\n",
    "using Pkg\n",
    "Pkg.activate(\".\")\n",
    "Pkg.instantiate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "using Plots\n",
    "using LaTeXStrings\n",
    "using Revise\n",
    "pyplot()\n",
    "# Set default fonts for all plots\n",
    "fnt = Plots.font(\"DejaVu Sans\", 8.0)\n",
    "default(titlefont=fnt, guidefont=fnt, tickfont=fnt, legendfont=fnt)\n",
    "\n",
    "function plotconvex()\n",
    "    x = 0:0.01:2.0\n",
    "    X=0.25;Y=1.75\n",
    "    f(x) = x^3.0\n",
    "    lambda = 0:0.01:1\n",
    "    x2 = [l*X + (1-l)*Y for l in lambda]\n",
    "    y2 = [l*f(X) + (1-l)*f(Y) for l in lambda]\n",
    "    y = f.(x)\n",
    "    p = plot(x,y, label=L\"f(\\lambda\\,x + (1-\\lambda)\\,y)\", linewidth=3)\n",
    "    plot!(x2,y2, label=L\"\\lambda\\, f(x) + (1-\\lambda)\\,f(y)\", linewidth=3)\n",
    "    annotate!(0.25, -0.65,text(\"x\", 18))\n",
    "    annotate!(1.75, -0.65,text(\"y\", 18))\n",
    "    return p\n",
    "end\n",
    "\n",
    "\n",
    "function plotinequalities(F,Xmax, op, lbl, col)\n",
    "    x = 0.0:0.1:Xmax\n",
    "    p = plot(xlim=(0,Xmax), ylim=(0,Xmax))\n",
    "    for i in 1:length(F)\n",
    "        y = F[i].(x)\n",
    "        if op[i] == 'L' \n",
    "            plot!(x,y,ylim=(0,Xmax),fill=(0, 0.3, col[i]), label = lbl[i]*\"<0\")\n",
    "        elseif op[i] == 'G'\n",
    "            plot!(x,y,ylim=(0,Xmax),fill=(Xmax, 0.3, col[i]), label = lbl[i]*\">0\")\n",
    "        else\n",
    "            error(\"Error\")\n",
    "        end\n",
    "    end    \n",
    "    return p\n",
    "end\n",
    "    \n",
    "\n",
    "function plotcheckloss(tau)\n",
    "    checkloss(x, tau) = tau*maximum([x,0.0]) + (1.0-tau)*maximum([-x, 0])\n",
    "    x = -1:0.01:1\n",
    "    y1 = checkloss.(x, tau)\n",
    "    p = plot(x,y1, label=L\"\\tau = \"*string(tau), linewidth=3, xlabel=\"z\", ylabel=L\"\\rho_\\tau(z)\")\n",
    "    \n",
    "    return p\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## MA934\n",
    "\n",
    "## Convex optimisation and linear programming\n",
    "\n",
    "\"The subject of linear programming is surrounded by notational and terminological thickets.\n",
    " Both of these defenses are lovingly cultivated by a coterie of stern acolytes who have devoted themselves to the field...\"\n",
    " \n",
    "         Numerical Recipes, Press et. al."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Reminder: gradient and Hessian\n",
    "\n",
    "Given a continuous, differentiable $f: \\mathbb{R}^n \\to \\mathbb{R}$, the *gradient* of $f$ at $\\mathbf{x}$ is the vector\n",
    "$$\n",
    "\\nabla\\,f(\\mathbf{x}) = \\left( \n",
    "\\begin{array}{c}\n",
    "\\frac{\\partial f(\\mathbf{x}) }{\\partial x_1}\\\\\n",
    "\\frac{\\partial f(\\mathbf{x}) }{\\partial x_2}\\\\\n",
    "\\vdots\\\\\n",
    "\\frac{\\partial f(\\mathbf{x}) }{\\partial x_n}\\\\\n",
    "\\end{array}\n",
    "\\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "Given a continuous, twice-differentiable $f: \\mathbb{R}^n \\to \\mathbb{R}$, the  *Hessian* of $f$ at $\\mathbf{x}$ is the matrix\n",
    "$$\n",
    "H\\,f(\\mathbf{x}) = \\left( \n",
    "\\begin{array}{ccc}\n",
    "\\frac{\\partial^2 f(\\mathbf{x}) }{\\partial x_1^2}& \\ldots & \\frac{\\partial^2 f(\\mathbf{x}) }{\\partial x_1 \\partial x_n}\\\\\n",
    "\\ldots & \\ldots & \\ldots\\\\\n",
    "\\frac{\\partial^2 f(\\mathbf{x}) }{\\partial x_n \\partial x_1} & \\ldots & \\frac{\\partial^2 f(\\mathbf{x}) }{\\partial x_n^2}\n",
    "\\end{array}\n",
    "\\right),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Convex sets\n",
    "\n",
    "$S \\subseteq \\mathbb{R}^n$ is a *convex set* if for any $x$, $y\\in S$, the line segment connecting $x$ and $y$ is contained in $S$. \n",
    "\n",
    "That is, for every point $z$,\n",
    "$$\n",
    "z = \\lambda\\, x + (1-\\lambda)\\,y\n",
    "$$\n",
    "with  $x$, $y\\in S$ and $\\lambda \\in [0,1]$, we have $z \\in S$. \n",
    "\n",
    "Intersection of convex sets is convex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<img src=\"files/images/convexset.png\" alt=\"convex set\" style=\"width: 250px;\"/> Convex set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<img src=\"files/images/nonconvexset.png\" alt=\"nonconvex set\" style=\"width: 250px;\"/> Nonconvex set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Convex functions\n",
    "\n",
    "Let $S \\subseteq \\mathbb{R}^n$ be a convex set and let $f: S \\to \\mathbb{R}$ be a real-valued function on $S$.\n",
    "\n",
    "$f$ is a *convex function* over $S$ if for\n",
    "any $x$, $y \\in S$ and $\\lambda \\in [0,1]$,\n",
    "$$\n",
    "f( \\lambda\\, x + (1-\\lambda)\\,y) \\leq  \\lambda\\, f(x) + (1-\\lambda)\\,f(y).\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "plotconvex()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Convex functions: alternative characterisation\n",
    "\n",
    "One can show that if $C \\subseteq \\mathbb{R}^n$ is a convex set and $f$ is a continuous, differentiable function on  $C$, then $f$ is convex on $C$ if and only if\n",
    "for all $\\mathbf{x}$, $\\mathbf{y} \\in C$ we have\n",
    "$$\n",
    "f(\\mathbf{y}) - f(\\mathbf{x}) \\geq \\nabla\\,f (\\mathbf{x}) \\cdot (\\mathbf{y} -\\mathbf{x}).\n",
    "$$\n",
    "\n",
    "Generalises the one-dimensional observation:\n",
    "$$\n",
    "\\frac{\\Delta f}{\\Delta x}(x,y) =\\frac{f(y)-f(x)}{y-x}  \\geq \\frac{df}{dx}(x)\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Optimisation\n",
    "\n",
    "Optimisation is the process of finding maxima and minima of a real-valued function, $f: \\mathbb{R}^n \\to \\mathbb{R}$, referred to as the objective function (cost function, optimand, loss function...) Various flavours:\n",
    "* local and global\n",
    "* constrained ($f: C \\subseteq \\mathbb{R}^n \\to \\mathbb{R}$) and unconstrained\n",
    "* convex and non-convex\n",
    "\n",
    "We will consider all problems to be minimisation problems. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Local and global minima\n",
    "\n",
    " Consider constrained optimisation problem over a set, $C \\subseteq \\mathbb{R}^n$:\n",
    " $$\\min_{\\substack{x \\in C} }f(x).$$\n",
    " \n",
    "A point $x^* \\in C$ is a *global minimum* of $f$ over $C$ if\n",
    "$$\n",
    "f(x^*) \\leq f(x) \\ \\ \\ \\text{for all $x \\in C$.}\n",
    "$$\n",
    "\n",
    "A point $x^* \\in C$ is a *local minimum* of $f$ over $C$ if there exists an $\\epsilon$-ball, $B(x^*, \\epsilon)$, centred at $x^*$ such that\n",
    "$$\n",
    "f(x^*) \\leq f(x) \\ \\ \\ \\text{for all $x \\in C \\cap B(x^*, \\epsilon)$,}\n",
    "$$\n",
    "with $\\epsilon > 0$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Convex optimisation \n",
    "\n",
    "A convex optimisation problem is a problem of the form\n",
    "$$\n",
    "\\min_{\\substack{x \\in C} }f(x)\n",
    "$$\n",
    "where $C$ is a convex set and $f$ is a convex function on $C$. \n",
    "\n",
    "Key result:\n",
    "> If $C$ is a convex set and $f$ is a convex function on $C$, then every local minimum of $f$ over $C$ is also a global minimum of $f$ over $C$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Optimality conditions for convex problems\n",
    "\n",
    "If $\\mathbf{x}^*$ is a local minimum of $f$, then \n",
    "$$\\nabla\\,f(\\mathbf{x}^*) = 0.$$\n",
    "Called *first order optimality conditions*. Necessary but not sufficient: eigenvectors of the Hessian to distinguish minima, maxima and saddles.\n",
    "\n",
    "However, for a convex function on a convex set $C$, for any $\\mathbf{y} \\in C$\n",
    "$$\n",
    "f(\\mathbf{y}) - f(\\mathbf{x}^*) \\geq \\nabla\\,f (\\mathbf{x}^*) \\cdot (\\mathbf{y} -\\mathbf{x}^*) = 0.\n",
    "$$\n",
    "So $f(\\mathbf{y}) \\geq f(\\mathbf{x}^*)$ for all $\\mathbf{y} \\in C$ and $\\mathbf{x}^*$ is a minimum.\n",
    "\n",
    "For convex optimisation problems, first order optimality conditions are necessary *and* sufficient.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### arg min notation\n",
    "\n",
    "Given an optimisation problem,\n",
    "$$\n",
    "\\min_{\\substack{x \\in C} }f(x)\n",
    "$$\n",
    "we will sometimes write the solution as\n",
    "$$\n",
    "x^* =  \\arg \\min_{x \\in C} f(x). \n",
    "$$\n",
    "\n",
    "This is shorthand  way of saying that $x^*$ is the value (or set of values) of $x$ that minimises $f(x)$ over the set $C$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Linear optimisation in $\\mathbb{R}$\n",
    "Consider minimising $f(x) = 2 x + 1$ with $x \\in \\mathbb{R}$. Clearly no minimum without constraints.\n",
    "\n",
    "Some possible constraints:\n",
    "1. $x \\leq 1$ and $x\\geq-1$,\n",
    "2. $x \\geq 1$ and $x\\leq -1$,\n",
    "3. $x\\leq -1$ and $x \\leq -2$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Outcomes:\n",
    "\n",
    "1. Solution exists at $x=-1$. \n",
    "2. No solution  because the constraints are inconsistent.\n",
    "3. No solution because the constraints do not prevent $x$ from becoming arbitrarily negative so $f(x)$ is unbounded below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Optimisation of linear functions in $\\mathbb{R^n}$\n",
    "\n",
    "Linear programming means optimisation of linear functions on $\\mathbb{R}^n$ subject to constraints.\n",
    "\n",
    " * $\\mathbf{x} = (x_1, x_2,\\ldots x_n) \\in \\mathbb{R}^n$ are called *structural variables*.\n",
    " *  $\\mathbf{c} = (c_1, c_2,\\ldots c_n) \\in \\mathbb{R}^n$ is called the *costs vector*.\n",
    " \n",
    " Objective function is\n",
    " $$\n",
    "\\begin{align}\n",
    "f:\\ \\mathbb{R}^n \\to \\mathbb{R} : \\ \\mathbf{x} &\\mapsto c_1 x_1 + c_2 x_2 + \\ldots + c_nx_n\\\\\n",
    "&= \\mathbf{c} \\cdot \\mathbf{x}.\n",
    "\\end{align}\n",
    "$$\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Optimisation of linear functions in $\\mathbb{R^n}$: constraints\n",
    "The task is to minimise or maximise $f(\\mathbf{x})$ subject to the $m$ constraints\n",
    "$$\n",
    "\\begin{align}\n",
    " a_{1 1}x_1 + a_{1 2}x_2 + \\ldots a_{1 n} x_n &\\sim b_1\\\\\n",
    "a_{2 1}x_1 + a_{2 2}x_2 + \\ldots a_{2 n} x_n &\\sim b_2\\\\\n",
    "\\vdots \\hspace{3.0cm} &\\vdots\\\\\n",
    "a_{m 1}x_1 + a_{m 2}x_2 + \\ldots a_{m n} x_n &\\sim b_m,\n",
    "\\end{align}\n",
    "$$\n",
    "(where $\\sim$ can mean $\\geq$, $\\leq$ or $=$)\n",
    "*and* $n$ *positivity conditions* on the structural variables,  \n",
    "$$\n",
    "x_1 \\geq 0,\\ x_2 \\geq 0, \\ldots x_n\\geq 0, \n",
    "$$\n",
    "There are $m+n$ constraints in total."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Feasible set\n",
    "\n",
    "$\\mathbf{x}$ is a *feasible* vector if it satisfies the constraints.\n",
    "\n",
    "The set of all feasible vectors is called the *feasible set*.\n",
    "A feasible vector, $\\mathbf{x}^*$, is said to be *optimal* if it minimises the objective function.\n",
    "\n",
    "There are 3 possibiliities:\n",
    "* an optimal feasible vector exists and the problem is said to be solvable,\n",
    "* there are no feasible vectors and the problem is said to be infeasible\n",
    "* feasible vectors exist but there is no minimum and the problem is said to be unbounded.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Constraints in $\\mathbb{R}^2$\n",
    "\n",
    "Sometimes easier to visualise the complement of the feasible set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "f1(x) = 50.0 - 2.0*x\n",
    "f2(x) = 30.0 - x/3.0\n",
    "f3(x) = 45.0 - 3.0*x\n",
    "f4(x) = 13.0 - x/3.0\n",
    "lbls = [L\"50-2x\", L\"30-\\frac{x}{3}\", L\"45 - 3x\", L\"13 - \\frac{x}{3}\"]\n",
    "cols = [:blue, :red, :green, :purple]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "plotinequalities([f1, f2, f3, f4], 40.0,['G','G','L', 'L'], lbls, cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Linear programming is a convex optimisation problem\n",
    "\n",
    "* Each equality constraint restricts the feasible set to a hyperplane of dimension n âˆ’ 1 in $\\mathbb{R}^n$.\n",
    "* Each inequality constraint defines a hyperplane dividing $\\mathbb{R}^n$ into an allowed and non-allowed half-space. \n",
    "* The feasible set is an intersection of convex sets and is thus convex (a polyhedron in $\\mathbb{R}^n$). \n",
    "* f (x) is a convex function on $\\mathbb{R}^n$. \n",
    "* All linear programming problems are therefore convex optimisation problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Slack variables\n",
    "\n",
    "An inequality constraint can be converted into an equality constraint by introducing an additional variable called a *slack variable*.\n",
    "\n",
    "$$\n",
    " a_{1}x_1 + a_{2}x_2 + \\ldots a_{n} x_n \\leq b.\n",
    "$$\n",
    "is equivalent to writing an equality involving a new, non-negative slack variable, $s$:\n",
    "$$\n",
    " a_{1}x_1 + a_{2}x_2 + \\ldots a_{n} x_n  + s =  b,\n",
    "$$\n",
    "with $s \\geq 0$. \n",
    "\n",
    "The positivity constraint on the slack variable measures the extent to which the original inequality is satisfied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Writing a linear program in standard form\n",
    "\n",
    "Any linear programme can be written in *standard form*  in which the only inequality constraints are the positivity conditions and all remaining constraints are equality constraints. A linear programme is in standard form if it in the form: minimise\n",
    "$$\n",
    "f(\\mathbf{x}) = \\mathbf{c}\\cdot\\,\\mathbf{x} = c_1 x_1 + c_2 x_2 + \\dots +c_{n}x_{n},\n",
    "$$\n",
    "subject to $m$ $equality$ constraints\n",
    "$$\n",
    "A\\,\\mathbf{x} = \\mathbf{b}\n",
    "$$\n",
    "and $n$ positivity conditions\n",
    "$$\n",
    "\\mathbf{x} \\geq 0\n",
    "$$\n",
    "where $A$ is an $m \\times n$ matrix, $\\mathbf{b}  \\in \\mathbb{R}^m$   and $\\mathbf{c} \\in \\mathbb{R}^{n}$  is a cost vector.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Converting a general linear programme to standard form\n",
    "\n",
    "1. Convert any $\\geq$ constraints to $\\leq$ constraints by multiplying by $-1$.\n",
    "2. For each $\\leq$ constraint introduce a new slack variable to convert it into an equality. Equality constraints do not need slack variables. \n",
    "3. Equality constraints with a negative right-hand side should be multiplied by -1 so that the right-hand side is positive.\n",
    "4. Build a new cost vector by padding the initial cost vector with zeros to accommodate the slack variables.\n",
    "\n",
    "Slack variables are now treated equivalently to the structural variables. \n",
    "However since they have zero cost, we can forget about them at the end.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The solution, $\\mathbf{x}^*$, of a solvable LP is at a vertex of the feasible set\n",
    "\n",
    "The gradient of the objective function, $f(\\mathbf{x})$, is nowhere zero in $\\mathbb{R}^n$:\n",
    "$$\n",
    "\\nabla f(\\mathbf{x}) = \\mathbf{c} \\neq 0.\n",
    "$$\n",
    "Thus $\\mathbf{x}^*$ cannot be in the interior of the feasible set, $S$, and must therefore be on the boundary. \n",
    "\n",
    "The boundary of a polyhedron of dimension $d$ is a set of faces which are polyhedra of dimension $d-1$.\n",
    "\n",
    "Let's refer to the face containing $\\mathbf{x}^*$ as $P$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The solution, $\\mathbf{x}^*$, of a solvable LP is at a vertex of the feasible set\n",
    "\n",
    "In general, the projection of the gradient of the objective function, $f(\\mathbf{x})$, onto the hyperplane containing $P$ is also nowhere zero in that hyperplane.\n",
    "\n",
    "Thus $\\mathbf{x}^*$ cannot be in the interior of $P$, and must be on the boundary. \n",
    "\n",
    "Iterating, we conclude that $\\mathbf{x}^*$ must be at a *vertex* of the feasible set.\n",
    "\n",
    "Since vertices are points in $\\mathbb{R}^n$, their coordinates must satisfy $n$ of the $n+m$ constraints as **equalities**.\n",
    "\n",
    "An $\\mathbf{x}$ satisfying $n$ constraints as equalities is called a *feasible basic vector*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Fundamental theorem of Linear Programming\n",
    "\n",
    "> For any LP, if an optimal feasible vector exists then there is a feasible basic vector that is optimal. \n",
    "\n",
    "This reduces the original optimisation problem to a finite combinatorial task: \n",
    "\n",
    ">  Enumerate all $\\binom{n+m}{n} $ feasible basic vectors, evaluate the objective function on each and choose a best one.\n",
    "\n",
    "This is then guaranteed to be optimal.\n",
    "\n",
    "Problem: in practice, the number of combinations can be prohibitively large.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Dantzig Simplex Algorithm\n",
    "\n",
    "* Dantzig simplex algorithm aims to determine *efficiently* which $n$ constraints from the $n+m$ available constraints are satisfied as equalities by the optimal feasible basic vector.\n",
    "\n",
    "* Idea: start from any vertex and iteratively move to new vertices so that the objective function always decreases.\n",
    "\n",
    "* Optimal vertex found when impossible to decrease any further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Basic and non-basic variables\n",
    "\n",
    "For a LP in standard form:\n",
    "\n",
    "* $m$ components of a basic feasible vector, $\\mathbf{x}$, are general non-zero  since $\\mathbf{x}$ satisfies the $m$ equality  constraints. These are called *basic variables*.\n",
    "* The remaining $n-m$ components of $\\mathbf{x}$ must be zero since $\\mathbf{x}$ must satisfy a subset of of the inequality constraints as equalities. These are called *non-basic variables*.\n",
    "* Different basic feasible vectors will have different basic and non-basic variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Simplex algorithm: schematic\n",
    "\n",
    "1. Start with any basic feasible vector, $\\mathbf{x}$. \n",
    "2. Identify the current non-basic variable, $x_k$, that would produce the largest decrease in the objective, $f(\\mathbf{x})$, if it became positive. \n",
    "3. Identify the current basic variable, $x_i$, that hits zero first as $x_k$ increases. \n",
    "4. Swap $x_i$ for $x_k$ to produce a new basic feasible vector, $\\mathbf{x}$ and return to step 2.\n",
    "\n",
    "To know how to select $x_k$ at step 2 and $x_i$ at step 3 some calculations are needed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Simplex algorithm: termination conditions\n",
    "\n",
    "How do we know when to stop? \n",
    "\n",
    "The algorithm can terminate in two ways:\n",
    "\n",
    "* At step 2: if none of the current non-basic variables can decrease $f(\\mathbf{x})$ by becoming positive, then the current basic feasible vector is optimal.\n",
    "\n",
    "* At step 3: if there is no basic variable that becomes zero as $x_k$ is increased, the problem is unbounded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Simplex algorithm: some notation\n",
    "\n",
    "\n",
    "Let us denote column $k$ of the matrix $A$ by $\\mathbf{a}_k$. Thus we have\n",
    "$$\n",
    "A = \\left(\\mathbf{a}_1 | \\mathbf{a}_2 | \\ldots | \\mathbf{a}_{n+m}\\right).\n",
    "$$\n",
    "Rearrange the columns of  $\\mathbf{x}^T$ so that the basic variables come first:\n",
    "$$\n",
    "\\mathbf{x}^T = (\\mathbf{x}_B\\, |\\, \\mathbf{x}_N).\n",
    "$$\n",
    "We must similarly rearrange the columns of $A$ and $\\mathbf{c}$:\n",
    "$$\n",
    "\\begin{align*}\n",
    "A & = \\left( A_B\\, |\\, A_N\\right)\\\\\n",
    "\\mathbf{c}^T & = \\left(\\mathbf{c}_B\\, | \\, \\mathbf{c}_N\\right).\n",
    "\\end{align*}\n",
    "$$\n",
    "Since the non-basic variables are all zero, $\\mathbf{x}_B$ satisfies\n",
    "$$\n",
    "\\begin{align}\n",
    "  & A_B\\,\\mathbf{x}_B = \\mathbf{b}\\\\\n",
    "\\Rightarrow &\\mathbf{x}_B = A_B^{-1}\\,\\mathbf{b}.\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Simplex algorithm: selecting the non-basic variable to make basic \n",
    "\n",
    "Take a non-basic variable, $x_k$ and increase it away from zero. \n",
    "Then $\\mathbf{x}_B$ changes to some other value, $\\mathbf{x}_B^\\prime$. \n",
    "Since only $x_k$ among the non-basic variables is not zero, the satisfaction of the constraints requires that\n",
    "$$\n",
    "A_B\\, \\mathbf{x}_B^\\prime + x_k\\,\\mathbf{a}_k = \\mathbf{b}.\n",
    "$$\n",
    "Solving for $\\mathbf{x}_B^\\prime$:\n",
    "$$\n",
    "\\mathbf{x}_B^\\prime = A_B^{-1}\\,\\mathbf{b} - x_k A_B^{-1}\\,\\mathbf{a}_k.\n",
    "$$\n",
    "Using the fact that $\\mathbf{x}_B = A_B^{-1}\\,\\mathbf{b}$ we get\n",
    "$$\n",
    "\\mathbf{x}_B^\\prime = \\mathbf{x}_B - x_k A_B^{-1}\\,\\mathbf{a}_k.\n",
    "$$\n",
    "We can now calculate the change in the objective function upon going from $\\mathbf{x}_B$ to $\\mathbf{x}_B^\\prime$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Simplex algorithm: selecting the non-basic variable to make basic \n",
    "\n",
    "Let $\\mathbf{e}_k$ be the unit vector in the $k^\\text{th}$ coordinate direction in $\\mathbb{R}^n$. \n",
    "\n",
    "**Before** increasing $x_k$, the objective function is:\n",
    "$$\n",
    "f[ \\left(\\mathbf{x}_B\\, | \\, \\mathbf{x}_N\\right)^T] = \\mathbf{c}_B^T\\,\\mathbf{x}_B.\n",
    "$$\n",
    "**After** increasing $x_k$ we have\n",
    "$$\n",
    "\\begin{align}\n",
    "f[ \\left(\\mathbf{x}_B^\\prime\\, | \\, \\mathbf{x}_N\\right) + x_k\\mathbf{e}_k] &= \\mathbf{c}_B^T\\,\\mathbf{x}_B^\\prime + c_k x_k\\\\\n",
    " & =  \\mathbf{c}_B^T \\left( \\mathbf{x}_B - x_k A_B^{-1}\\,\\mathbf{a}_k \\right) + c_k x_k\\\\\n",
    " &=  \\mathbf{c}_B^T\\,\\mathbf{x}_B - x_k\\left( \\mathbf{c}_B^T A_B^{-1}\\mathbf{a}_k -c_k\\right).\n",
    "\\end{align}\n",
    "$$ \n",
    "The **difference** is\n",
    "$$\n",
    "(\\Delta\\,f)_k = x_k\\left( c_k - \\mathbf{c}_B^T A_B^{-1}\\mathbf{a}_k\\right).\n",
    "$$\n",
    "We choose the non-basic variable, $x_k$, with the **most negative** value of $(\\Delta \\, f)_k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Simplex algorithm: selecting the basic variable to make non-basic \n",
    "We have found above\n",
    "$$\n",
    "\\mathbf{x}_B^\\prime = \\mathbf{x}_B - x_k A_B^{-1}\\,\\mathbf{a}_k.\n",
    "$$\n",
    "\n",
    "To simplify notation, write $\\mathbf{w} = A_B^{-1}\\,\\mathbf{a}_k$. The $i^\\text{th}$ component of $\\mathbf{x}_B^\\prime$ is\n",
    "$$\n",
    "(x_B^\\prime)_i = (x_B)_i - x_k\\,w_i.\n",
    "$$\n",
    "We see that for each $w_i > 0$, the $i^\\text{th}$ component of $\\mathbf{x}_B^\\prime$ decreases as $x_k$ increases.\n",
    "Setting $(x_B^\\prime)_i =0$ and solving for $x_k$ we get\n",
    "$$\n",
    "x_k = \\frac{(x_B)_i }{w_i}.\n",
    "$$\n",
    "We choose the basic variable with the smallest value of this ratio (among those with $w_i > 0$) since it reaches zero first as $x_k$ increases. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Finding an initial basic feasible vector: simple case\n",
    "\n",
    "We have not yet shown how to find a basic feasible vector to start the simplex algorithm. Sometimes is is easy. Consider this example in $\\mathbb{R}^2$:\n",
    "$$\n",
    "\\begin{align*}\n",
    "2\\,x_1 + x_2 &\\leq 70\\\\\n",
    "x_1 + 3\\,x_2 &\\leq 90, \n",
    "\\end{align*}\n",
    "$$\n",
    "with $x_1 \\geq 0$ and  $x_2 \\geq 0$. \n",
    "\n",
    "Introducing slack variables, $s_1$ and $s_2$, gives a LP in standard form in $\\mathbb{R}^4$: \n",
    "$$\n",
    "\\begin{align*}\n",
    "2\\,x_1 + x_2 + s_1 &= 70\\\\\n",
    "x_1 + 3\\,x_2 + s_2 & = 90, \n",
    "\\end{align*}\n",
    "$$\n",
    "with $x_1 \\geq 0$, $x_2 \\geq 0$, $s_1 \\geq 0$ and $s_2 \\geq 0$. \n",
    "Setting $x_1 = x_2 = 0$, immediately gives a basic feasible vector: $(x_1, x_2, s_1, s_2 ) = (0,0, 70, 90)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Finding an initial basic feasible vector: general case\n",
    "\n",
    "This simple approach can fail for two reasons. \n",
    "\n",
    "1. if an inequality has a negative right-hand side after we put the problem in standard form. Setting the original variables to zero would give a negative slack variable and violate the positivity constraint. \n",
    "2. if there are equality constraints, they don't have a slack variable in standard form and would generally not be satisfied if the original variables are set to zero. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Finding an initial basic feasible vector: general case\n",
    "\n",
    "Here is an example in $\\mathbb{R}^3$:\n",
    "$$\n",
    "\\begin{align*}\n",
    "-3\\,x_1 + x_2 + x_3 &\\geq 1\\\\\n",
    "x_1 + 2\\,x_2 - x_3 &\\leq -2\\\\\n",
    "2\\,x_1 + x_2 &\\leq 2\\\\\n",
    "-x_1 - x_2 &= -1,\n",
    "\\end{align*}\n",
    "$$\n",
    "with  $x_1 \\geq 0$, $x_2 \\geq 0$ and  $x_3 \\geq 0$. In standard form in $\\mathbb{R}^6$ we have\n",
    "$$\n",
    "\\begin{align*}\n",
    "3\\,x_1 - x_2 - x_3 +s_1 &= -1\\\\\n",
    "x_1 + 2\\,x_2 - x_3  + s_2 &= -2\\\\\n",
    "2\\,x_1 + x_2 + s_3 & = 2\\\\\n",
    "x_1 + x_2 &= 1,\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Finding an initial basic feasible vector: artificial variables\n",
    "\n",
    "The general approach is to introduce a *second* set of new variables called *artificial variables*.\n",
    "\n",
    "Artificial variables make it easy to find a feasible basic vector, albeit for a *modified* linear programme to the one we started with, called the *auxiliary problem*. \n",
    "\n",
    "The objective function of the auxiliary problem is constructed so that the artifial variables are zero at the optimum.\n",
    "\n",
    "Using the simplex method to solve the auxiliary problem finds a basic feasible vector of the original problem since the artificial variables are set to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Finding an initial basic feasible vector: artificial variables\n",
    "\n",
    "* Any constraint with negative right hand side that came from an inequality will already contain a slack variable, $s$, in standard form:\n",
    "$$\n",
    " a_{k 1}x_1 + a_{k 2}x_2 + \\ldots a_{k n} x_n + s = - b_k .\n",
    "$$\n",
    "We subtract an artificial variable, $a$, from the left-hand side to give\n",
    "$$\n",
    " a_{k 1}x_1 + a_{k 2}x_2 + \\ldots a_{k n} x_n +s - a = - b_k,\n",
    "$$\n",
    "with $a \\geq 0$.\n",
    "\n",
    "* For any equality constraint in standard form,\n",
    "$$\n",
    " a_{k 1}x_1 + a_{k 2}x_2 + \\ldots a_{k n} x_n = b_k,\n",
    "$$\n",
    "we add an artificial variable,  $a$, to the left-hand side:\n",
    "$$\n",
    " a_{k 1}x_1 + a_{k 2}x_2 + \\ldots a_{k n} x_n  + a =  b_k,\n",
    "$$\n",
    "with $a \\geq 0$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Finding an initial basic feasible vector: artificial variables\n",
    "\n",
    "In both cases, we have removed the obstruction to finding a basic feasible vector since we can now write $x_1 = x_2 = \\ldots = x_n = 0$, $s=0$ (if necessary) and $a= b_k > 0$.\n",
    "\n",
    "The objective function of the auxiliary problem should be the sum of all of the artificial variables introduced.\n",
    "\n",
    "Minimising this objective function via the simplex algorithm will find an optimum at which the artificial variables are zero which gives a basic feasible vector for the original LP.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### JuMP\n",
    "\n",
    "* [JuMP](https://jump.dev/JuMP.jl/v0.19.0/index.html) is a framework for mathematical optimisation in Julia. \n",
    "* Provides unified access to lots of optimisation algorithms.\n",
    "* Preferred option for research use.\n",
    "\n",
    "Let's use JuMP to solve a simple linear programme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using JuMP\n",
    "using GLPK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GLPK is a Julia wrapper for the GNU Linear Programming Kit C library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### JuMP : solving a simple linear programme\n",
    "\n",
    "$$\\min_{\\substack{(x_1, x_2) \\in \\mathbb{R}^2} } -40\\, x_1 - 60\\, x_2$$\n",
    "\n",
    "subject to the constraints\n",
    "\n",
    "$$2\\, x_1 + x_2 \\leq 70 $$\n",
    "$$x_1 + 3\\, x_2 \\leq 90 $$\n",
    "$$ 3\\, x_1 + x_2 \\geq 46 $$\n",
    "$$ x_1 + 4\\, x_2 \\geq 52 $$\n",
    "\n",
    "with $x_1 \\geq 0$ and $x_2 \\geq 0$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### JuMP\n",
    "\n",
    "We need to create a \"model\", then specify the variables, objective function and constraints "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Model(with_optimizer(GLPK.Optimizer))\n",
    "@variable(m, x>= 0) \n",
    "@variable(m, y >= 0) \n",
    "@objective(m, Min, -40x - 60y)\n",
    "@constraint(m, const1,  2x + y <=  70)\n",
    "@constraint(m, const2,   x + 3y <= 90)\n",
    "@constraint(m, const3,  3x + y >=  46)\n",
    "@constraint(m, const4,   x + 4y >= 52)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### JuMP\n",
    "\n",
    "Now we call the optimiser to solve the problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "optimize!(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "Some bespoke functions are provided by JuMP to access the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(\"Status = \", termination_status(m))\n",
    "println(\"Optimal Objective Function value: \", objective_value(m))\n",
    "println(\"Optimal Solutions:\")\n",
    "println(\"x = \", value(x))\n",
    "println(\"y = \", value(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Application: quantile regression\n",
    "\n",
    "Observations, $\\left\\{(x_i, y_i), i=1 \\ldots n \\right\\}$ \n",
    "\n",
    "Linear regression models the conditional expectation of $y$ given $x$ as a linear function:\n",
    "$$\n",
    "\\mathbb{E}(y\\, |\\, x) = \\beta_0 + \\beta_1\\, x.\n",
    "$$\n",
    "\n",
    "Get $\\mathbf{\\beta} = (\\beta_0, \\beta_1)$,by minimising the sum of squared errors:\n",
    "\n",
    "$$ \\sum_{i=1}^n \\left(y_i - \\beta_0 - \\beta_1\\,x_i \\right)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Quantile regression models the $\\tau$-quantile of $y$ given $x$ as a linear function:\n",
    "$$\n",
    "\\mathbb{Q}_\\tau(y\\, |\\, x) = \\beta_0(\\tau) + \\beta_1(\\tau)\\, x.\n",
    "$$\n",
    "\n",
    "$\\mathbf{\\beta} = (\\beta_0, \\beta_1)$, estimated by minimising the sum of *tilted losses*:\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^n \\rho_\\tau \\left(y_i - \\beta_0 - \\beta_1\\,x_i \\right).\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\rho_\\tau(z) = \\left|z \\right| \\left(\\tau \\mathbb{1}_{(z\\geq 0)} + (1-\\tau)\\mathbb{1}_{(z<0)}\\right).\n",
    "$$\n",
    "$\\mathbb{1}_{S}$ is the [indicator function](https://en.wikipedia.org/wiki/Indicator_function)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Loss function for quantile regression\n",
    "\n",
    "Why this strange optimand of reweighted absolute errors?\n",
    "\n",
    "Consider, for example, the 25% quantile, $\\tau=0.25$.\n",
    "\n",
    "The $\\beta$'s should be such that 25% of the residuals are negative and 75% are positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "p = plotcheckloss(0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "* -ve residuals upweighted by $1-\\tau = 0.75$.\n",
    "* +ve residuals downweighted by $\\tau = 0.25$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Quantile regression as a linear programme\n",
    "\n",
    "An equivalent way to write the tilted loss function (check) is\n",
    "$$\n",
    "\\rho_\\tau(z) = \\tau\\, \\max(z, 0) + (1-\\tau)\\,\\max(-z,0).\n",
    "$$\n",
    "\n",
    "Write the residuals as\n",
    "$$ y_i - \\beta_0 - \\beta_1\\,x_i = u_i^+ - u_i^-$$\n",
    "where \n",
    "$$\n",
    "\\begin{align*}\n",
    "u^+_i  &= \\max(y_i - \\beta_0 - \\beta_1\\,x_i,\\,0) \\geq 0\\\\\n",
    "u^-_i & = \\max(\\beta_0 + \\beta_1\\,x_i-y_i,\\,0) \\geq 0.\n",
    "\\end{align*}\n",
    "$$\n",
    "We then have\n",
    "\n",
    "$$\n",
    "\\rho_\\tau(y_i - \\beta_0 - \\beta_1\\,x_i) = \\tau u_i^+ + (1-\\tau)\\,u_i^-\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Quantile regression as a linear programme\n",
    "\n",
    "The quantile regression optimisation problem is then equivalent to the LP:\n",
    "$$\n",
    "\\min_{\\beta_0, \\beta_1, u^\\pm_1\\ldots u^\\pm_n} \\sum_{i=1}^n \\tau u_i^+ + (1-\\tau)\\,u_i^-\n",
    "$$\n",
    "subject to the $n$ constraints\n",
    "$$\n",
    "u_i^+ - u_i^- = y_i - \\beta_0 - \\beta_1\\,x_i  \\ \\ \\ \\ \\ \\ \\ \\text{for } i=1\\ldots n,\n",
    "$$\n",
    "and the positivity conditions \n",
    "$$\n",
    "u_i^+\\geq 0,\\ \\  u_i^- \\geq 0  \\ \\ \\ \\ \\ \\ \\ \\text{for } i=1\\ldots n.\n",
    "$$\n",
    "\n",
    "To put in standard form, also need to write\n",
    "$$ \n",
    "\\beta_0 = \\beta_0^+ + \\beta_0^- \\ \\ \\ \\text{and}\\ \\ \\ \n",
    "\\beta_1 = \\beta_1^+ + \\beta_1^-\n",
    "$$\n",
    "with $\\beta_0^+ \\geq 0$, $\\beta_0^- \\geq 0$, $\\beta_1^+ \\geq 0$, $\\beta_1^- \\geq 0$."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Julia 1.5.1",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
