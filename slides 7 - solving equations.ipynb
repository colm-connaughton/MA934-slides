{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up environment with correct dependencies\n",
    "using Pkg\n",
    "Pkg.activate(\".\")\n",
    "Pkg.instantiate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots\n",
    "using LaTeXStrings\n",
    "using Revise\n",
    "using LinearAlgebra\n",
    "using DifferentialEquations\n",
    "pyplot()\n",
    "# Set default fonts for all plots\n",
    "fnt = Plots.font(\"DejaVu Sans\", 5.0)\n",
    "default(titlefont=fnt, guidefont=fnt, tickfont=fnt, legendfont=fnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## MA934\n",
    "\n",
    "## Solving equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Root-finding in 1D\n",
    "\n",
    "Task: find $x$ such that $f(x) = 0$.\n",
    "\n",
    "Interval $[a_0,b_0]$ brackets a root of $f(x)$ if $f(a_0)\\,f(b_0) <0$.\n",
    "\n",
    "Bracket and bisect method:\n",
    "\n",
    "1 $c_n = \\frac{1}{2}(a_n+b_n)$  \n",
    "2 If $f(a_n)\\,f(c_n) <0$, $[a_{n+1}, b_{n+1}] = [a_n, c_n]$. Otherwise  $[a_{n+1}, b_{n+1}] = [c_n, b_n]$  \n",
    "3  Repeat until $\\epsilon_n = b_n-a_n < \\epsilon_\\text{tol}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "include(\"files/code/figures.jl\")\n",
    "figures.plot_bracket_root()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "Convergence: $\\epsilon_n \\sim 2^{-n}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Root-finding in 1D: Newton-Raphson iteration\n",
    "\n",
    "Suppose root is at $x=x^*$ and current estimate is $x_n$.\n",
    "\n",
    "Write $x_{n+1} = x_n+\\delta_n$ and try to choose $\\delta_n$ such that\n",
    "$f(x_{n+1}) = 0$:\n",
    "\n",
    "Taylor expand:\n",
    "$$\n",
    "0 = f(x_{n+1}) = f(x_n + \\delta_n) = f(x_n) + \\delta_n\\,f^\\prime(x_n) + \\mathcal{O}(\\delta_n^2) \n",
    "$$\n",
    "Assuming we are near the root, we neglect the $\\mathcal{O}(\\delta_n^2)$ terms and solve for $\\delta_n$: \n",
    "$$\n",
    "\\delta_n = -\\frac{f(x_n)}{f^\\prime(x_n)}.\n",
    "$$\n",
    "This gives the Newton-Raphson iterative method:\n",
    "$$\n",
    "x_{n+1} = x_n  -\\frac{f(x_n)}{f^\\prime(x_n)}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Convergence properties of Newton-Raphson\n",
    "\n",
    "Geometrical interpretation:\n",
    "\n",
    "<img src=\"files/images/Newton-Raphson.jpg\" alt=\"NR\" style=\"width: 400px;\"/>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "NR is not guaranteed to converge:\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"files/images/Newton-RaphsonFailure1.jpg\" alt=\"NR\" style=\"width: 400px;\"/>  \n",
    "\n",
    "\n",
    "\n",
    "<img src=\"files/images/Newton-RaphsonFailure2.jpg\" alt=\"NR\" style=\"width: 400px;\"/>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Super-exponential convergence  of Newton-Raphson\n",
    "\n",
    "When NR does converge, it converges super-exponentially.\n",
    "\n",
    "Let  $\\epsilon_n = x_n - x_*$.  Then NR formula gives:\n",
    "$$\n",
    "\\epsilon_{n+1} = \\epsilon_n - \\frac{f(x^*+\\epsilon_n)}{f^\\prime(x^*+\\epsilon_n)}.\n",
    "$$\n",
    "Taylor expanding and using $f(x^*)=0$ one obtains (check):\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "f(x^*+\\epsilon_n) &= \\epsilon_n\\, f^\\prime(x_*)\\,\\left( 1+ \\frac{\\epsilon_n}{2}\\frac{f^{\\prime\\prime}(x_*)}{f^\\prime(x_*)} \\right) + O(\\epsilon_n^3),\\\\\n",
    "f^\\prime(x^*+\\epsilon_n) &=  f^\\prime(x_*)\\, \\left( 1+ \\epsilon_n \\frac{f^{\\prime\\prime}(x_*)}{f^\\prime(x_*)} \\right) + O(\\epsilon_n^2).\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Super-exponential convergence of Newton-Raphson\n",
    "\n",
    "Keeping only the leading order terms on the RHS, there is a cancellation at $\\mathcal{O}(\\epsilon_n)$ and we get\n",
    "$$\n",
    " \\epsilon_{n+1} = \\alpha\\,\\epsilon _n^2.\n",
    "$$\n",
    "where $\\alpha =  \\frac{f^{\\prime\\prime}(x_*)}{2\\,f^\\prime(x_*)}$ depends on the properties of $f$ at the root, $x^*$.\n",
    "\n",
    "This nonlinear recursion relation can be linearised by $a_n = \\log_\\alpha \\epsilon_n$ to give:\n",
    "$$\n",
    "a_{n+1} = 2\\,a_n+1\n",
    "$$\n",
    "Solution is $a_n = c_0\\,2^{n-1}-1$ where $c_0$ is a constant.  \n",
    "In original variables, choosing $c_0$ to match the initial condition, $\\epsilon_0$, we get\n",
    "$$\n",
    "\\epsilon_n = \\epsilon_0\\,\\left|\\alpha \\right|^{-1}\\,\\left|\\alpha \\right|^{2^n}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Newton-Raphson in $\\mathbb{R}^n$\n",
    "\n",
    "System of $n$ equations in $\\mathbb{R}^n$:\n",
    "$$\n",
    "\\begin{align*}\n",
    "F_1(x_1\\ldots x_n) &= 0 \\\\\n",
    "\\vdots\\hspace{1.0cm}  & \\ \\  \\vdots\\\\\n",
    "F_n(x_1\\ldots x_n) &= 0. \n",
    "\\end{align*}\n",
    "$$\n",
    "or \n",
    "$$\n",
    "\\mathbf{F}(\\mathbf{x}) = 0.\n",
    "$$\n",
    "\n",
    "As before, write $\\mathbf{x}_{n+1}=\\mathbf{x}_n+\\boldsymbol{\\delta}_n$ and try to choose  $\\boldsymbol{\\delta}_n$  such that  $\\mathbf{F}(\\mathbf{x}_{n+1})=0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "Taylor expand:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "0 &= \\mathbf{F}(\\mathbf{x}_{n+1}) = \\mathbf{F}(\\mathbf{x}_n + \\boldsymbol{\\delta}_n)\\\\\n",
    "&= \\mathbf{F}(\\mathbf{x}_n) + \\mathbf{J}(\\mathbf{x}_n)\\,\\boldsymbol{\\delta}_n + \\mathcal{O}(\\left|\\boldsymbol{\\delta}\\right|_n^2) \n",
    "\\end{align*},\n",
    "$$\n",
    "\n",
    "where $\\mathbf{J}(\\mathbf{x}_n)$ is the Jacobian matrix\n",
    "\n",
    "$$\n",
    "\\mathbf{J}_{i\\,j} = \\frac{\\partial F_i}{\\partial x_j}(\\mathbf{x}_n).\n",
    "$$\n",
    "\n",
    "Neglecting the $\\mathcal{O}(\\left|\\boldsymbol{\\delta}_n\\right|^2)$ terms, we  obtain $\\boldsymbol{\\delta}_n$ from a linear solve:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\delta}_n = - \\mathbf{J}(\\mathbf{x}_n)\\,\\backslash  \\, \\mathbf{F}(\\mathbf{x}_n)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: Newton-Raphson iteration in $\\mathbb{C}$.\n",
    "\n",
    "For complex valued functions of a complex variable:\n",
    "$$\n",
    "z_{n+1} = z_n - \\frac{f(z_n)}{f^\\prime(z_n)}\n",
    "$$\n",
    "\n",
    "Nonlinear iterated map which can lead to very rich dynamics (periodic cycles, chaos, intermittency etc). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "Example: compute basins of attraction in $\\mathbb{C}$  of the roots of the polynomial\n",
    "$$\n",
    "f(z) = z^3-1 =0.\n",
    "$$\n",
    "under NR iteration.\n",
    "\n",
    "<img src=\"files/images/newton-fractal.png\" alt=\"NR\" style=\"width: 400px;\"/>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Ordinary differential equations\n",
    "\n",
    "For initial value problems, it is generally sufficient to develop algorithms to solve autonomous first order systems in $\\mathbb{R}^n$: \n",
    "$$\n",
    "\\frac{d \\mathbf{u}}{d t}=  \\mathbf{F}(\\mathbf{u}) \\hspace{0.5cm}\\text{with }\\hspace{0.5cm} \\mathbf{u}(0) = \\mathbf{U}_0.\n",
    "$$\n",
    "For example, the second order non-autonomous equation,\n",
    "$$\n",
    "\\frac{d^2 y}{d t^2} + 2 \\nu \\omega \\frac{d y}{d t} + \\omega^2 y = F(t),\n",
    "$$\n",
    "is equivalent (check) to this 3-dimensional autonomous system:\n",
    "$$\n",
    "\\frac{d }{d t} \\left(\n",
    "\\begin{array}{c}\n",
    "u^{(1)}\\\\ \n",
    "u^{(2)}\\\\ \n",
    "u^{(3)} \n",
    "\\end{array}\n",
    "\\right)\n",
    "= \n",
    "\\left(\n",
    "\\begin{array}{c}\n",
    "-2 \\nu \\omega u^{(1)} - \\omega^2 u^{(2)} + F(u^{(3)})\\\\ \n",
    "u^{(1)}\\\\ \n",
    "1 \n",
    "\\end{array}\n",
    "\\right)\n",
    "\\hspace{0.5cm}\\text{where }\\hspace{0.5cm} \\left(\n",
    "\\begin{array}{c}\n",
    "u^{(1)}\\\\ \n",
    "u^{(2)}\\\\ \n",
    "u^{(3)} \n",
    "\\end{array}\n",
    "\\right) = \\left(\n",
    "\\begin{array}{c}\n",
    "\\frac{d y}{d t}\\\\ \n",
    "y\\\\ \n",
    "t \n",
    "\\end{array}\n",
    "\\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Discretisation and time stepping\n",
    "\n",
    "**Discretisation**: approximate continuous $\\mathbf{u}(t)$ on $t\\in [0,T]$ by\n",
    "$\\{\\mathbf{u}_i\\ :\\ i=0\\ldots N\\}$. Here \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{u}_i &= \\mathbf{u}(t_i)\\\\\n",
    "t_i &=i\\,h,\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "and the \"time step\" is\n",
    "\n",
    "$$\n",
    "h=\\frac{T}{N}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "If $h$ is \"small\" then\n",
    "\n",
    "$$\n",
    "\\frac{d \\mathbf{u}}{d t}=  \\mathbf{F}(\\mathbf{u}) \\hspace{0.5cm}\\text{with }\\hspace{0.5cm} \\mathbf{u}(0) = \\mathbf{U}_0.\n",
    "$$\n",
    "\n",
    "can be hueristically approximated by\n",
    "\n",
    "$$\n",
    "\\frac{\\mathbf{u}_{i+1}-\\mathbf{u}_i}{h} = \\mathbf{F}(\\mathbf{u}_i)\n",
    "\\hspace{0.25cm}\\text{with }\\hspace{0.25cm} \\mathbf{u}_0 = \\mathbf{U}_0.\n",
    "$$\n",
    "\n",
    "Re-arranging, we get the (Forward Euler) time-stepping algorithm\n",
    "\n",
    "$$\n",
    "\\mathbf{u}_{i+1} = \\mathbf{u}_i + h\\,\\mathbf{F}_i \\hspace{0.25cm}\\text{with }\\hspace{0.25cm} \\mathbf{u}_0 = \\mathbf{U}_0,\n",
    "$$\n",
    "\n",
    "where $\\mathbf{F}_i$ means $\\mathbf{F}(\\mathbf{u}_i)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Taylor's theorem\n",
    "\n",
    "\n",
    "If $f(x)$ is a real-valued function which is differentiable $n+1$ times on the interval $[x, x+h]$ then there exists a point, $\\xi$, in $[x, x+h]$ such that\n",
    "$$\n",
    "f(x+h) = f(x) + \\frac{1}{1!}\\, h \\, \\frac{d f}{d x}(x) + \\frac{1}{2!}\\, h^2 \\, \\frac{d^2f}{d x^2}(x) + \\ldots\\\\\n",
    "+ \\frac{1}{n!}\\, h^n \\, \\frac{d^nf}{d x^n}(x) + h^{n+1} R_{n+1}(\\xi)\n",
    "$$\n",
    "where\n",
    "$$\n",
    "R_{n+1}(\\xi) = \\frac{1}{(n+1)!} \\, \\frac{d^{n+1} f}{d x^{n+1}}(\\xi).\n",
    "$$\n",
    "Useful for systematic analysis of discrete approximations to derivatives and ODEs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Finite difference approximations of derivatives\n",
    "\n",
    "Approximate $f^\\prime(x)$ by linear combinations of values of $f$ at nearby points.  \n",
    "Taylor with $n=1$:\n",
    "$$\n",
    "f(x+h) = f(x) + h \\, f^\\prime(x) + \\mathcal{O}(h^2).\n",
    "$$\n",
    "Using same discrete notation as before, rearrange to get:\n",
    "$$\n",
    "f^\\prime(x_i) = \\frac{f_{i+1} - f_i}{h} +\\mathcal{O}(h). \\hspace{1.0cm}\\text{Forward difference formula.}\n",
    "$$\n",
    "Could also have started from $f(x-h)$ to obtain:\n",
    "$$\n",
    "f^\\prime(x_i) = \\frac{f_{i} - f_{i-1}}{h} +\\mathcal{O}(h). \\hspace{1.0cm}\\text{Backwards difference formula.}\n",
    "$$\n",
    "In both cases, the approximation error is $\\mathcal{O}(h)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Higher order finite differences: improving the error\n",
    "\n",
    "Improved accuracy can be obtained by linearly combining more points:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "f_{i+1} &= f_i + h\\,f^\\prime(x_i) +  \\frac{1}{2}\\, h^2 \\, f^{\\prime\\prime}(x_i) + \\mathcal{O}(h^3)\\\\\n",
    "f_{i-1} &= f_i - h\\,f^\\prime(x_i) +  \\frac{1}{2}\\, h^2 \\, f^{\\prime\\prime}(x_i) + \\mathcal{O}(h^3).\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Take the linear combination $a_1\\,f_{i-1} +a_2\\,f_i +a_3\\,f_{i+1}$:\n",
    "\n",
    "$$\n",
    "a_1\\,f_{i-1} +a_2\\,f_i +a_3\\,f_{i+1} = (a_1+a_2+a_3)\\,f_i + (a_3-a_1)\\,h\\,f^\\prime(x_i)\\\\\n",
    "+ \\frac{1}{2}\\,(a_3+a_1)\\, h^2 \\, f^{\\prime\\prime}(x_i) + O(h^3).\n",
    "$$\n",
    "We want to choose the $a$'s to cancel the terms proportional to $h^0$ and $h^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Higher order finite differences: improving the error\n",
    "\n",
    "$a_1$, $a_2$ and $a_3$ should therefore satisfy the equations\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "a_1+a_2+a_3 &=0\\\\\n",
    "a_3-a_1 &= 1\\\\\n",
    "a_3+a_1 &=0.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "We get $a_1=-\\frac{1}{2}$, $a_2=0$ and $a_3 = \\frac{1}{2}$.\n",
    "\n",
    "Rearranging to get an expression for $f^\\prime(x_i)$:\n",
    "$$\n",
    "f^\\prime(x_i) = \\frac{f_{i+1} - f_{i-1}}{2\\,h} + O(h^2).  \\hspace{1.0cm}\\text{Centred difference formula.}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Higher order finite differences: improving the error\n",
    "\n",
    "The set of points underpinning a finite-difference approximation is known as the \"stencil\". \n",
    "\n",
    "A 5-point stencil leads to a 4th order accurate finite difference formula for $f^\\prime(x_i)$:\n",
    "\n",
    "$$\n",
    "\\frac{-f_{i+2}\\!+\\!8 f_{i+1}\\! - \\!8 f_{i-1}\\!+\\!f_{i-2}}{12\\,h}\\! +\\! O(h^4).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "<img src=\"files/images/finiteDifferences.jpg\" alt=\"FD\" style=\"width: 400px;\"/> \n",
    "\n",
    "Error as a function of $h$ for several finite difference approximations to the derivative of $f(x) = \\sqrt{x}$ at $x=2$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Euler method again\n",
    "\n",
    "Return to the ODE (assumed scalar from now on)\n",
    "$$\n",
    "\\frac{d u}{d t}=  F(u) \\hspace{0.5cm}\\text{with }\\hspace{0.5cm} u(0) = U_0.\n",
    "$$\n",
    "Accounting for the error in the forward difference approximation,\n",
    "\n",
    "$$\n",
    "\\frac{u_{i+1}-u_i}{h} +\\mathcal{O}(h) = F(u_i)\n",
    "\\hspace{0.25cm}\\text{with }\\hspace{0.25cm} u_0 = U_0.\n",
    "$$\n",
    "\n",
    "The Forward Euler time-stepping algorithm thus has *step-wise error* $\\mathcal{O}(h^2)$:\n",
    "\n",
    "$$\n",
    "u_{i+1} = u_i + h\\,F_i  +\\mathcal{O}(h^2)\\hspace{0.25cm}\\text{with }\\hspace{0.25cm} \\mathbf{u}_0 = \\mathbf{U}_0,\n",
    "$$\n",
    "\n",
    "*Global error* is $\\mathcal{O}(N h^2) = T\\,\\mathcal{O}(h)$ since $h=\\frac{T}{N}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Implicit methods: Backward Euler\n",
    "\n",
    "We could equally have used the backward difference approximation:\n",
    "\n",
    "$$\n",
    "\\frac{u_{i}-u_{i-1}}{h} +\\mathcal{O}(h) = F(u_i)\n",
    "\\hspace{0.25cm}\\text{with }\\hspace{0.25cm} u_0 = U_0.\n",
    "$$\n",
    "\n",
    "which gives (with $i \\to i+1$)\n",
    "\n",
    "$$\n",
    "u_{i+1} = u_i + h\\, F_{i+1} + O(h^2).\n",
    "$$\n",
    "\n",
    "This is called the Backward Euler method. Note $u_{i+1}$ appears on both sides. \n",
    "\n",
    "This is an example of an *implicit* method. Implicit methods require a Newton-Raphson solve at each timestep.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Improving accuracy: trapezoidal method\n",
    "\n",
    "Another way to think about time-stepping algorithms is via approximation of\n",
    "$$\n",
    "u(t+h) = u(t) + \\int_t^{t+h} \\!\\!\\!\\!F(u(\\tau))\\,d\\tau.\n",
    "$$\n",
    "\n",
    "Approximation of integral with left and right Riemann rule gives the forward and backward Euler methods respectively (check)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "If we use the Trapezoidal rule we get:\n",
    "\n",
    "$$\n",
    "u_{i+1} = u_i + \\frac{h}{2}\\left[F_i + F_{i+1} \\right].\n",
    "$$\n",
    "This is called the implicit trapezoidal method.\n",
    "\n",
    "The implicit trapezoidal method improves on the Euler methods because it has a stepwise error of $\\mathcal{O}(h^3)$. How to show this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Error analysis of timestepping rules\n",
    "\n",
    "Consider Taylor expansion of $u_{i+1}$:\n",
    "$$\n",
    "u_{i+1} = u_i + h \\frac{d u}{d t}(t_i) + \\frac{h^2}{2}\\frac{d^2 u}{d t^2}(t_i) + O(h^3)\\\\\n",
    "= u_i + h F_i +\\frac{1}{2} h^2 F_i\\, F^\\prime_i +  O(h^3)\n",
    "$$\n",
    "Note the use of the chain rule:\n",
    "$$\n",
    "\\frac{d^2 u}{d t^2}(t_i) = \\left.\\frac{d }{d t}F(u(t))\\right|_{t=t_i} = \\left. F^\\prime(u(t))\\,\\frac{d u}{d t}\\right|_{t=t_i}\\\\\n",
    "= \\left. F^\\prime(u(t))\\,F(u(t))\\right|_{t=t_i} = F^\\prime_i\\,F_i.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Error analysis of timestepping rules\n",
    "\n",
    "Idea: Taylor expand RHS of timestepping rule and identify the first order at which expansion differs from the above.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "F(u(t_{i+1})) &= F(u_i + h F_i + \\mathcal{O}(h^2))\\\\\n",
    "&= F(u_i) + (h F_i) \\frac{d F}{d u}(u(t_i)) + O(h^2)\\\\\n",
    "&=F_i + h F_i F_i^\\prime + O(h^2).\n",
    "\\end{align*}\n",
    "$$\n",
    "RHS of implicit trapezoidal method is therefore\n",
    "$$\n",
    "u_i + \\frac{h}{2}\\left[F_i + F_{i+1}\\right] =  u_i + h F_i +\\frac{1}{2} h^2 F_i\\, F^\\prime_i +  \\mathcal{O}(h^3)\n",
    "$$\n",
    "Comparing with $u_{i+1}$, stepwise error is $\\mathcal{O}(h^3)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Predictor-corrector methods\n",
    "\n",
    "Can we get higher order accuracy with an explicit scheme?\n",
    "\n",
    "**Predictor-corrector** idea: use a less accurate explicit method to predict $u_{i+1}$ and then use the higher order formula to correct this prediction.\n",
    "\n",
    "Example: improved Euler method:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "1. Use forward Euler as predictor:\n",
    "$$\n",
    "u^*_{i+1} = u_i + h F_i,\n",
    "$$\n",
    "and calculate\n",
    "$$\n",
    "F^*_{i+1} = F(u^*_{i+1}).\n",
    "$$\n",
    "2. Use the Trapezoidal Method to correct this :\n",
    "$$\n",
    "u_{i+1} = u_i + \\frac{h}{2}\\left[F_i + F^*_{i+1} \\right].\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Error analysis (similar to above - check) shows stepwise error is $\\mathcal{O}(h^3)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Choosing the timestep\n",
    "\n",
    "In practice we need to operate at a finite value of $h$. How do we choose it? \n",
    "\n",
    "Measure the error by comparing the numerical solution at a grid point, $\\widetilde{u}_i$, to the exact solution, $u(t_i)$. \n",
    "\n",
    "Two tolerance criteria are commonly used:\n",
    "$$\n",
    "\\begin{align*}\n",
    "&E_a(h) = \\left| \\widetilde{u}_i - u_i \\right| \\leq \\epsilon &  \\text{Absolute error threshold,}\\\\\n",
    "&E_r(h) = \\frac{\\left| \\widetilde{u}_i - u_i \\right|}{\\left| u_i\\right|} \\leq \\epsilon &  \\text{Relative error threshold.}\n",
    "\\end{align*}\n",
    "$$\n",
    "Fixing error tolerance allows us to take larger timesteps when the solution is varying slowly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Adaptive timestepping : estimating error\n",
    "\n",
    "Problem: exact solution usually not known. Use trial steps:\n",
    "\n",
    "1. Take a trial step with stepsize $h$ from $u_i$ to get estimate $u^{\\rm B}_{i+1}$. \n",
    "2. Take 2 trial steps with stepsize $\\frac{h}{2}$ from $u_i$ to get estimate $u^{\\rm S}_{i+1}$. \n",
    "3. Estimate of local error is\n",
    "$$\n",
    "\\Delta = \\left|u^{\\rm B}_{i+1} - u^{\\rm S}_{i+1} \\right|.\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Adaptive timestepping : selecting the new stepsize\n",
    "\n",
    "If timestepping method is $n^{th}$ order, we know there is a constant $c$ such that\n",
    "\n",
    "$$\n",
    "c\\, h_{\\text{old}}^n = \\Delta.\n",
    "$$\n",
    "\n",
    "For maximum efficiency, we should choose $h_{\\text{new}}$ to saturate the error threshold:\n",
    "\n",
    "$$\n",
    "c\\,h_{\\text{new}}^n  = \\epsilon.\n",
    "$$\n",
    "\n",
    "Eliminating $c$ between these two equations gives:\n",
    "\n",
    "$$\n",
    "h_{\\text{new}} = \\left(\\frac{\\epsilon}{\\Delta}\\right)^\\frac{1}{n}h_{\\text{old}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Stiff problems\n",
    "\n",
    "With adaptive stepsizing we expect to take larger timesteps when the solution is varying slowly and smaller timesteps when the solution is varying rapidly.\n",
    "\n",
    "Sometimes explicit methods result in very small step sizes even when the solution is varying *slowly*. Such problems are said to be *stiff*.\n",
    "\n",
    "Efficient solution of stiff problems requires bespoke stiff solvers that are usually implicit.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Stiff problems\n",
    "\n",
    "Computational stiffness is a complicated topic: depends on the equation, the initial condition, the numerical method being used and the interval of integration. See [this article](http://www.scholarpedia.org/article/Stiff_systems).\n",
    "\n",
    "Common feature is that the solution is varying slowly but \"nearby\" solutions vary rapidly.\n",
    "\n",
    "The simple equation \n",
    "\n",
    "$$\n",
    "\\frac{d u }{d t} = -\\lambda\\, u \\hspace{0.5cm} \\text{with }u(0) = 1,\n",
    "$$\n",
    "\n",
    "turns out to have this property when $\\lambda \\gg 1$ or solution interval is long."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Solving ODEs in Julia\n",
    "\n",
    "[DifferentialEquations.jl](https://docs.juliadiffeq.org/stable/) is a well developed system for solving systems of differential equations. In its basic form, it is very simple to use. \n",
    "\n",
    "Let's solve the *relaxation oscillator* equations:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{d x}{d t} &= \\mu( y -(\\frac{1}{3} x^3 - x))\\\\\n",
    "\\frac{d y}{d t} &= -\\frac{1}{\\mu}x.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "**Steps**:\n",
    "\n",
    "1. Define the RHS of the system of equations.\n",
    "2. Define an ```ODEProblem``` object.\n",
    "3. Integrate the ```ODEProblem```.\n",
    "4. Plot and analyse the solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Step 1: define the RHS \n",
    "\n",
    "* ```du``` is the right hand side of the system (as a vector)\n",
    "* ```u``` are the dependent variables for (as a vector)\n",
    "* ```t``` is the time variable (for non-autonomous systems)\n",
    "* ```p``` is a list of parameters that need to be passed in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "using DifferentialEquations\n",
    "\n",
    "function rel_osc!(du,u,p,t)\n",
    "    x,y = u\n",
    "    μ = p[1]\n",
    "    du[1] = μ *(y - ((x^3.0)/3.0 - x ))\n",
    "    du[2] = - x / μ\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Step 2: define the problem\n",
    "* specify initial condition, ```u0```.\n",
    "* specify solution interval, ```tspan```\n",
    "* provide values for the parameters, ```p```\n",
    "* create an ```ODEProblem``` object. Takes ```u0```, ```tspan```, ```p``` and the RHS function from step 1 as arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "u0 = [-0.1,-0.1]\n",
    "tspan = (0.0,25.0)\n",
    "p = [1.0]\n",
    "prob = ODEProblem(rel_osc!,u0,tspan,p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Step 3: integrate the equations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sol=solve(prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Step 4: plot the solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "plot(sol,vars=(1,2), label=\"(x(t), y(t))\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "plot(sol, vars=(0,1), label=\"x(t)\")\n",
    "plot!(sol,vars=(0,2), label=\"y(t)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Comparison of Forward and Backward Euler for decay equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "function exp_decay(u,p,t)\n",
    "    λ = p[1]\n",
    "    return -λ*u\n",
    "end\n",
    "\n",
    "u0 = 100.0; λ = 10.0;T=20.0/λ\n",
    "h = 0.15\n",
    "tspan = (0.0,T)\n",
    "p = [λ]\n",
    "prob = ODEProblem(exp_decay,u0,tspan,p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "include(\"files/code/figures.jl\")\n",
    "sol1  = solve(prob, Euler(), dt=h)\n",
    "sol2  = solve(prob, ImplicitEuler(), dt=h, adaptive=false)\n",
    "figures.plot_Euler(sol1, sol2, λ, u0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Two timescales in relaxation oscillator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "# Try changing μ\n",
    "μ = 5.0\n",
    "p1, p2 = figures.plot_relaxation_oscillator(μ);\n",
    "p1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "p2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Julia 1.5.1",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
